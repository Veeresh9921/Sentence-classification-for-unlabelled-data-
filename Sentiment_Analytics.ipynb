{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f0d99dd",
   "metadata": {},
   "source": [
    "# Sentiment Analysis  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4faacaf8",
   "metadata": {},
   "source": [
    "Steps\n",
    "\n",
    "1)Load the dataset \n",
    "\n",
    "2)Clean Dataset\n",
    "\n",
    "3)Encode Sentiments\n",
    "\n",
    "4)Split Dataset\n",
    "\n",
    "5)Tokenize and Pad/Truncate Reviews\n",
    "\n",
    "6)Build Architecture/Model\n",
    "\n",
    "7)Train and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75159db7",
   "metadata": {},
   "source": [
    "#                            #Approach one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daf048f",
   "metadata": {},
   "source": [
    "# Import all the libraries needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd7bf8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from unidecode import unidecode\n",
    "from nltk.corpus import stopwords\n",
    "from re import sub\n",
    "import pandas as pd\n",
    "import time\n",
    "import multiprocessing\n",
    "from tensorflow.keras.models import load_model\n",
    "# !pip install -U -q segmentation-models\n",
    "# !pip install -q tensorflow==2.1\n",
    "# !pip install -q keras==2.3.1\n",
    "# !pip install -q tensorflow-estimator==2.1.\n",
    "import re\n",
    "## Imports libs\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "from tensorflow import keras\n",
    "import segmentation_models as sm\n",
    "from tensorflow.keras.models import Sequential \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c81b19",
   "metadata": {},
   "source": [
    "# Loading Data and Preview dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c668e15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r'C:\\Users\\veereshg\\Downloads\\Dataset.csv',encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93dfe8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>MEMBER_ID</th>\n",
       "      <th>REASONNPSSCORE__C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a2p1U000000RowfQAC</td>\n",
       "      <td>0011U00000rjFKdQAM</td>\n",
       "      <td>I showed up for my appointment, but they had m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a2p1U000000RqQqQAK</td>\n",
       "      <td>0011U00000riCSHQA2</td>\n",
       "      <td>Staff was polite, courteous, and on time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a2p1U000000RqXyQAK</td>\n",
       "      <td>0011U00000riTw7QAE</td>\n",
       "      <td>Overall care is great!  It's wonderful to be a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a2p1U000000Rq1LQAS</td>\n",
       "      <td>0011U00000rhu8eQAA</td>\n",
       "      <td>Like the doctor and staff at this location. Ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a2p1U000000RpiuQAC</td>\n",
       "      <td>0011U00000rk4SHQAY</td>\n",
       "      <td>The convenience and the doctors</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID           MEMBER_ID  \\\n",
       "0  a2p1U000000RowfQAC  0011U00000rjFKdQAM   \n",
       "1  a2p1U000000RqQqQAK  0011U00000riCSHQA2   \n",
       "2  a2p1U000000RqXyQAK  0011U00000riTw7QAE   \n",
       "3  a2p1U000000Rq1LQAS  0011U00000rhu8eQAA   \n",
       "4  a2p1U000000RpiuQAC  0011U00000rk4SHQAY   \n",
       "\n",
       "                                   REASONNPSSCORE__C  \n",
       "0  I showed up for my appointment, but they had m...  \n",
       "1           Staff was polite, courteous, and on time  \n",
       "2  Overall care is great!  It's wonderful to be a...  \n",
       "3  Like the doctor and staff at this location. Ea...  \n",
       "4                    The convenience and the doctors  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b23a319f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3812 entries, 0 to 3811\n",
      "Data columns (total 3 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   ID                 3812 non-null   object\n",
      " 1   MEMBER_ID          3812 non-null   object\n",
      " 2   REASONNPSSCORE__C  3812 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 89.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331469e0",
   "metadata": {},
   "source": [
    "Dropping the features which are not adding value to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be33eafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['ID','MEMBER_ID'],inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cf5204",
   "metadata": {},
   "source": [
    "# Creating labels to the unlabelled data using vader_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dbf1cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = {'REASONNPSSCORE__C':'review'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71f1797e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(keep='first',inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af1b8ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\veereshg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"vader_lexicon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f84a206d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###USING nltk SENTIMENT VANDER TO LABEL THE SENTENCE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ee6deec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def vader_sentiment_result(sent):\n",
    "    scores = analyzer.polarity_scores(sent)\n",
    "    \n",
    "    if scores[\"neg\"] > scores[\"pos\"]:\n",
    "        return 0\n",
    "\n",
    "    return 1\n",
    "\n",
    "df[\"sentiment\"] = df[\"review\"].apply(lambda x: vader_sentiment_result(x))\n",
    "#df[\"vader_result\"] = valid_set[\"review\"].apply(lambda x: vader_sentiment_result(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "804f2a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I showed up for my appointment, but they had m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Staff was polite, courteous, and on time</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Overall care is great!  It's wonderful to be a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Like the doctor and staff at this location. Ea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The convenience and the doctors</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  I showed up for my appointment, but they had m...          1\n",
       "1           Staff was polite, courteous, and on time          1\n",
       "2  Overall care is great!  It's wonderful to be a...          1\n",
       "3  Like the doctor and staff at this location. Ea...          1\n",
       "4                    The convenience and the doctors          1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d638e92",
   "metadata": {},
   "source": [
    "# Processing Data\n",
    "\n",
    "Load and Clean Dataset\n",
    "In the original dataset, the reviews are still dirty. There are still numbers, uppercase, and punctuations. This will not be good for training, so in load_dataset() function, beside loading the dataset using pandas, I also pre-process the reviews by removing  non alphabet (punctuations and numbers), stop words, and lower case all of the reviews.\n",
    "\n",
    "Stop Word is a commonly used words in a sentence, usually a search engine is programmed to ignore this words (i.e. \"the\", \"a\", \"an\", \"of\", etc.)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a6427cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "  \n",
    "    ls = WordNetLemmatizer()\n",
    "    x_data = df['review']       # Reviews/Input\n",
    "    y_data = df['sentiment']    # Sentiment/Output\n",
    "\n",
    "    # PRE-PROCESS REVIEW\n",
    "    \n",
    "    x_data = x_data.replace({'<.*?>': ''}, regex = True)          # remove html tag\n",
    "    x_data = x_data.replace({'[^A-Za-z]': ' '}, regex = True)\n",
    "    x_data = x_data.apply(lambda review: [ls.lemmatize(w) for w in review.split() if w not in stopwords])\n",
    "    x_data = x_data.apply(lambda review: [w.lower() for w in review])\n",
    "      # remove stop words\n",
    "      # lower case\n",
    "    \n",
    "    # ENCODE SENTIMENT -> 0 & 1\n",
    "   \n",
    "\n",
    "    return x_data, y_data\n",
    "\n",
    "\n",
    "# print('Reviews')\n",
    "# print(x_data, '\\n')\n",
    "# # print('Sentiment')\n",
    "# print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1baa70b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data,y_data=load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61704a62",
   "metadata": {},
   "source": [
    "# Split Dataset\n",
    "\n",
    "\n",
    "In this work, I decided to split the data into 80% of Training and 20% of Testing set using train_test_split method from Scikit-Learn. By using this method, it automatically shuffles the dataset. We need to shuffle the data because in the original dataset, the reviews and sentiments are in order, where they list positive reviews first and then negative reviews. By shuffling the data, it will be distributed equally in the model, so it will be more accurate for predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b19bc58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.2)\n",
    "\n",
    "# print('Train Set')\n",
    "# print(x_train, '\\n')\n",
    "# print(x_test, '\\n')\n",
    "# print('Test Set')\n",
    "# print(y_train, '\\n')\n",
    "# print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3522361e",
   "metadata": {},
   "source": [
    "Function for getting the maximum review length, by calculating the mean of all the reviews length (using numpy.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebced4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length():\n",
    "    review_length = []\n",
    "    for review in x_train:\n",
    "        review_length.append(len(review))\n",
    "\n",
    "    return int(np.ceil(np.mean(review_length)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f65a813",
   "metadata": {},
   "source": [
    "# Tokenize and Pad/Truncate Reviews\n",
    "\n",
    "A Neural Network only accepts numeric data, so we need to encode the reviews. I use tensorflow.keras.preprocessing.text.Tokenizer to encode the reviews into integers, where each unique word is automatically indexed (using fit_on_texts method) based on x_train.\n",
    "x_train and x_test is converted into integers using texts_to_sequences method.\n",
    "\n",
    "Each reviews has a different length, so we need to add padding (by adding 0) or truncating the words to the same length (in this case, it is the mean of all reviews length) using tensorflow.keras.preprocessing.sequence.pad_sequences.\n",
    "\n",
    "post, pad or truncate the words in the back of a sentence\n",
    "pre, pad or truncate the words in front of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7f86fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded X Train\n",
      " [[  11    2    3 ...    0    0    0]\n",
      " [   1   11  144 ...    4  571    3]\n",
      " [  14    9  129 ...  251 1963   55]\n",
      " ...\n",
      " [  88   10    9 ...    0    0    0]\n",
      " [ 626    2  329 ...    0    0    0]\n",
      " [ 480    8    5 ...   27 1080 1364]] \n",
      "\n",
      "Encoded X Test\n",
      " [[   1   81  457 ...    0    0    0]\n",
      " [   1   68  908 ...   67  220  186]\n",
      " [ 353  355   48 ...    1  914   66]\n",
      " ...\n",
      " [ 170 2172 1252 ...    0    0    0]\n",
      " [  16   17   20 ...    0    0    0]\n",
      " [   1  373    5 ... 2683   19    6]] \n",
      "\n",
      "Maximum review length:  12\n"
     ]
    }
   ],
   "source": [
    "# ENCODE REVIEW\n",
    "token = Tokenizer(lower=False)    # no need lower, because already lowered the data in load_data()\n",
    "token.fit_on_texts(x_train)\n",
    "x_train = token.texts_to_sequences(x_train)\n",
    "x_test = token.texts_to_sequences(x_test)\n",
    "\n",
    "max_length = get_max_length()\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')\n",
    "x_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "total_words = len(token.word_index) + 1   # add 1 because of 0 padding\n",
    "\n",
    "print('Encoded X Train\\n', x_train, '\\n')\n",
    "print('Encoded X Test\\n', x_test, '\\n')\n",
    "print('Maximum review length: ', max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191ca534",
   "metadata": {},
   "source": [
    "# Build Architecture/Model\n",
    "Embedding Layer: in simple terms, it creates word vectors of each word in the word_index and group words that are related or have similar meaning by analyzing other words around them.\n",
    "\n",
    "LSTM Layer: to make a decision to keep or throw away data by considering the current input, previous output, and previous memory. There are some important components in LSTM.\n",
    "\n",
    "Forget Gate, decides information is to be kept or thrown away\n",
    "Input Gate, updates cell state by passing previous output and current input into sigmoid activation function\n",
    "Cell State, calculate new cell state, it is multiplied by forget vector (drop value if multiplied by a near 0), add it with the output from input gate to update the cell state value.\n",
    "Ouput Gate, decides the next hidden state and used for predictions\n",
    "Dense Layer: compute the input with the weight matrix and bias (optional), and using an activation function. I use Sigmoid activation function for this work because the output is only 0 or 1.\n",
    "\n",
    "The optimizer is Adam and the loss function is Binary Crossentropy because again the output is only 0 and 1, which is a binary number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc379253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 12, 32)            115264    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 140,161\n",
      "Trainable params: 140,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# ARCHITECTURE\n",
    "EMBED_DIM = 32\n",
    "LSTM_OUT = 64\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, EMBED_DIM, input_length = max_length))\n",
    "model.add(LSTM(LSTM_OUT))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7396cb",
   "metadata": {},
   "source": [
    "# Training\n",
    "For training, it is simple. We only need to fit our x_train (input) and y_train (output/label) data. For this training, I use a mini-batch learning method with a batch_size of 64 and 50 epochs.\n",
    "\n",
    "Also, I added a callback called checkpoint to save the model locally for every epoch if its accuracy improved from the previous epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cfbc5b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('LSTM.h5',monitor='accuracy',save_best_only=True,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "183060fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0264 - accuracy: 0.9926\n",
      "\n",
      "Epoch 00001: accuracy improved from 0.99025 to 0.99260, saving model to LSTM.h5\n",
      "Epoch 2/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0187 - accuracy: 0.9963\n",
      "\n",
      "Epoch 00002: accuracy improved from 0.99260 to 0.99630, saving model to LSTM.h5\n",
      "Epoch 3/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0147 - accuracy: 0.9980\n",
      "\n",
      "Epoch 00003: accuracy improved from 0.99630 to 0.99798, saving model to LSTM.h5\n",
      "Epoch 4/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0183 - accuracy: 0.9956\n",
      "\n",
      "Epoch 00004: accuracy did not improve from 0.99798\n",
      "Epoch 5/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0261 - accuracy: 0.9916\n",
      "\n",
      "Epoch 00005: accuracy did not improve from 0.99798\n",
      "Epoch 6/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0170 - accuracy: 0.9939\n",
      "\n",
      "Epoch 00006: accuracy did not improve from 0.99798\n",
      "Epoch 7/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0227 - accuracy: 0.9916\n",
      "\n",
      "Epoch 00007: accuracy did not improve from 0.99798\n",
      "Epoch 8/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0100 - accuracy: 0.9987\n",
      "\n",
      "Epoch 00008: accuracy improved from 0.99798 to 0.99866, saving model to LSTM.h5\n",
      "Epoch 9/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0085 - accuracy: 0.9976\n",
      "\n",
      "Epoch 00009: accuracy did not improve from 0.99866\n",
      "Epoch 10/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0087 - accuracy: 0.9980\n",
      "\n",
      "Epoch 00010: accuracy did not improve from 0.99866\n",
      "Epoch 11/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0095 - accuracy: 0.9970\n",
      "\n",
      "Epoch 00011: accuracy did not improve from 0.99866\n",
      "Epoch 12/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0087 - accuracy: 0.9983\n",
      "\n",
      "Epoch 00012: accuracy did not improve from 0.99866\n",
      "Epoch 13/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0084 - accuracy: 0.9973\n",
      "\n",
      "Epoch 00013: accuracy did not improve from 0.99866\n",
      "Epoch 14/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0097 - accuracy: 0.9973\n",
      "\n",
      "Epoch 00014: accuracy did not improve from 0.99866\n",
      "Epoch 15/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0111 - accuracy: 0.9963\n",
      "\n",
      "Epoch 00015: accuracy did not improve from 0.99866\n",
      "Epoch 16/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0198 - accuracy: 0.9939\n",
      "\n",
      "Epoch 00016: accuracy did not improve from 0.99866\n",
      "Epoch 17/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0106 - accuracy: 0.9976\n",
      "\n",
      "Epoch 00017: accuracy did not improve from 0.99866\n",
      "Epoch 18/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0088 - accuracy: 0.9976\n",
      "\n",
      "Epoch 00018: accuracy did not improve from 0.99866\n",
      "Epoch 19/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0080 - accuracy: 0.9980\n",
      "\n",
      "Epoch 00019: accuracy did not improve from 0.99866\n",
      "Epoch 20/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0098 - accuracy: 0.9976\n",
      "\n",
      "Epoch 00020: accuracy did not improve from 0.99866\n",
      "Epoch 21/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0109 - accuracy: 0.9970\n",
      "\n",
      "Epoch 00021: accuracy did not improve from 0.99866\n",
      "Epoch 22/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0092 - accuracy: 0.9976\n",
      "\n",
      "Epoch 00022: accuracy did not improve from 0.99866\n",
      "Epoch 23/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0052 - accuracy: 0.9990\n",
      "\n",
      "Epoch 00023: accuracy improved from 0.99866 to 0.99899, saving model to LSTM.h5\n",
      "Epoch 24/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0056 - accuracy: 0.9990\n",
      "\n",
      "Epoch 00024: accuracy did not improve from 0.99899\n",
      "Epoch 25/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0054 - accuracy: 0.9987\n",
      "\n",
      "Epoch 00025: accuracy did not improve from 0.99899\n",
      "Epoch 26/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0044 - accuracy: 0.9990\n",
      "\n",
      "Epoch 00026: accuracy did not improve from 0.99899\n",
      "Epoch 27/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0045 - accuracy: 0.9993\n",
      "\n",
      "Epoch 00027: accuracy improved from 0.99899 to 0.99933, saving model to LSTM.h5\n",
      "Epoch 28/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0033 - accuracy: 0.9993\n",
      "\n",
      "Epoch 00028: accuracy did not improve from 0.99933\n",
      "Epoch 29/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0107 - accuracy: 0.9970\n",
      "\n",
      "Epoch 00029: accuracy did not improve from 0.99933\n",
      "Epoch 30/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0080 - accuracy: 0.9973\n",
      "\n",
      "Epoch 00030: accuracy did not improve from 0.99933\n",
      "Epoch 31/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0057 - accuracy: 0.9990\n",
      "\n",
      "Epoch 00031: accuracy did not improve from 0.99933\n",
      "Epoch 32/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0043 - accuracy: 0.9990\n",
      "\n",
      "Epoch 00032: accuracy did not improve from 0.99933\n",
      "Epoch 33/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0034 - accuracy: 0.9993\n",
      "\n",
      "Epoch 00033: accuracy did not improve from 0.99933\n",
      "Epoch 34/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0052 - accuracy: 0.9987\n",
      "\n",
      "Epoch 00034: accuracy did not improve from 0.99933\n",
      "Epoch 35/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0036 - accuracy: 0.9993\n",
      "\n",
      "Epoch 00035: accuracy did not improve from 0.99933\n",
      "Epoch 36/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0042 - accuracy: 0.9990\n",
      "\n",
      "Epoch 00036: accuracy did not improve from 0.99933\n",
      "Epoch 37/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0038 - accuracy: 0.9987\n",
      "\n",
      "Epoch 00037: accuracy did not improve from 0.99933\n",
      "Epoch 38/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0036 - accuracy: 0.9993\n",
      "\n",
      "Epoch 00038: accuracy did not improve from 0.99933\n",
      "Epoch 39/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0030 - accuracy: 0.9993\n",
      "\n",
      "Epoch 00039: accuracy did not improve from 0.99933\n",
      "Epoch 40/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0031 - accuracy: 0.9993\n",
      "\n",
      "Epoch 00040: accuracy did not improve from 0.99933\n",
      "Epoch 41/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0029 - accuracy: 0.9990\n",
      "\n",
      "Epoch 00041: accuracy did not improve from 0.99933\n",
      "Epoch 42/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0027 - accuracy: 0.9993\n",
      "\n",
      "Epoch 00042: accuracy did not improve from 0.99933\n",
      "Epoch 43/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0026 - accuracy: 0.9990\n",
      "\n",
      "Epoch 00043: accuracy did not improve from 0.99933\n",
      "Epoch 44/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0027 - accuracy: 0.9993\n",
      "\n",
      "Epoch 00044: accuracy did not improve from 0.99933\n",
      "Epoch 45/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0028 - accuracy: 0.9993\n",
      "\n",
      "Epoch 00045: accuracy did not improve from 0.99933\n",
      "Epoch 46/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0023 - accuracy: 0.9993\n",
      "\n",
      "Epoch 00046: accuracy did not improve from 0.99933\n",
      "Epoch 47/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0023 - accuracy: 0.9993\n",
      "\n",
      "Epoch 00047: accuracy did not improve from 0.99933\n",
      "Epoch 48/50\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0026 - accuracy: 0.9993\n",
      "\n",
      "Epoch 00048: accuracy did not improve from 0.99933\n",
      "Epoch 49/50\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.0022 - accuracy: 0.9990\n",
      "\n",
      "Epoch 00049: accuracy did not improve from 0.99933\n",
      "Epoch 50/50\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0024 - accuracy: 0.9990\n",
      "\n",
      "Epoch 00050: accuracy did not improve from 0.99933\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1de8d57c3a0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size = 64, epochs = 50, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fcf4a4",
   "metadata": {},
   "source": [
    "# Testing\n",
    "To evaluate the model, we need to predict the sentiment using our x_test data and comparing the predictions with y_test (expected output) data. Then, we calculate the accuracy of the model by dividing numbers of correct prediction with the total data. Resulted an accuracy of 84.67%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26295aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\veereshg\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:454: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Prediction: 631\n",
      "Wrong Prediction: 113\n",
      "Accuracy: 84.81182795698925\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_classes(x_test, batch_size = 128)\n",
    "\n",
    "true = 0\n",
    "for i, y in enumerate(y_test):\n",
    "    if y == y_pred[i]:\n",
    "        true += 1\n",
    "\n",
    "print('Correct Prediction: {}'.format(true))\n",
    "print('Wrong Prediction: {}'.format(len(y_pred) - true))\n",
    "print('Accuracy: {}'.format(true/len(y_pred)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c1c64a",
   "metadata": {},
   "source": [
    "# Load Saved Model\n",
    "Load saved model and use it to predict a movie review statement's sentiment (positive or negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "59c06f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model('LSTM.h5')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4ab2cac5",
   "metadata": {},
   "source": [
    "Receives a review as an input to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fda5474a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hospital Review: Which types of hospital is this, very bad experience , don't go here, you know why , cus of discharge time of patients was today's evening 5-6 oclk and this hospital without giving any basic details of asking the charges of amount .\n"
     ]
    }
   ],
   "source": [
    "review = str(input('Hospital Review: '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "05d9e4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned:  Which types of hospital is this very bad experience  dont go here you know why  cus of discharge time of patients was todays evening  oclk and this hospital without giving any basic details of asking the charges of amount \n",
      "Filtered:  ['which types hospital bad experience  dont go know  cus discharge time patients todays evening  oclk hospital without giving basic details asking charges amount ']\n"
     ]
    }
   ],
   "source": [
    "# Pre-process input\n",
    "regex = re.compile(r'[^a-zA-Z\\s]')\n",
    "review = regex.sub('', review)\n",
    "print('Cleaned: ', review)\n",
    "\n",
    "words = review.split(' ')\n",
    "filtered = [w for w in words if w not in stopwords]\n",
    "filtered = ' '.join(filtered)\n",
    "filtered = [filtered.lower()]\n",
    "\n",
    "print('Filtered: ', filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1800eda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 881 1002  233   25  808   36   44    3 2714 1722 1002  244]]\n"
     ]
    }
   ],
   "source": [
    "tokenize_words = token.texts_to_sequences(filtered)\n",
    "tokenize_words = pad_sequences(tokenize_words, maxlen=max_length, padding='post', truncating='post')\n",
    "print(tokenize_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "41768b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.11768821]]\n"
     ]
    }
   ],
   "source": [
    "result = loaded_model.predict(tokenize_words)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6adb26",
   "metadata": {},
   "source": [
    "If the confidence score is close to 0, then the statement is negative. On the other hand, if the confidence score is close to 1, then the statement is positive. I use a threshold of 0.7 to determine which confidence score is positive and negative, so if it is equal or greater than 0.7, it is positive and if it is less than 0.7, it is negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0f4f02e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n"
     ]
    }
   ],
   "source": [
    "if result >= 0.7:\n",
    "    print('positive')\n",
    "else:\n",
    "    print('negative')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991906e8",
   "metadata": {},
   "source": [
    "# Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3b84addc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.47      0.49       116\n",
      "           1       0.90      0.92      0.91       628\n",
      "\n",
      "    accuracy                           0.85       744\n",
      "   macro avg       0.71      0.69      0.70       744\n",
      "weighted avg       0.84      0.85      0.84       744\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a75437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e927fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98b9986",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e2cc43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e771185",
   "metadata": {},
   "source": [
    "# SECOND APPROACH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6095cca",
   "metadata": {},
   "source": [
    "# Using transformer based zero shot classification to label the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d54b8d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli (https://huggingface.co/facebook/bart-large-mnli)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "58333420",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r'C:\\Users\\veereshg\\Downloads\\Dataset.csv',encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c495236",
   "metadata": {},
   "source": [
    "# Applying single shot classification on every sentence to label the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d5e7debe",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_labels = [\"positive\", \"negative\"]\n",
    "DF['results']=df.REASONNPSSCORE__C.apply(lambda text : classifier(text, the_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cb908d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.to_csv('NLPdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0639c083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.3</th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>MEMBER_ID</th>\n",
       "      <th>REASONNPSSCORE__C</th>\n",
       "      <th>results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a2p1U000000RowfQAC</td>\n",
       "      <td>0011U00000rjFKdQAM</td>\n",
       "      <td>I showed up for my appointment, but they had m...</td>\n",
       "      <td>{'sequence': \"I showed up for my appointment, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>a2p1U000000RqQqQAK</td>\n",
       "      <td>0011U00000riCSHQA2</td>\n",
       "      <td>Staff was polite, courteous, and on time</td>\n",
       "      <td>{'sequence': 'Staff was polite, courteous, and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>a2p1U000000RqXyQAK</td>\n",
       "      <td>0011U00000riTw7QAE</td>\n",
       "      <td>Overall care is great!  It's wonderful to be a...</td>\n",
       "      <td>{'sequence': \"Overall care is great!  It's won...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>a2p1U000000Rq1LQAS</td>\n",
       "      <td>0011U00000rhu8eQAA</td>\n",
       "      <td>Like the doctor and staff at this location. Ea...</td>\n",
       "      <td>{'sequence': 'Like the doctor and staff at thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>a2p1U000000RpiuQAC</td>\n",
       "      <td>0011U00000rk4SHQAY</td>\n",
       "      <td>The convenience and the doctors</td>\n",
       "      <td>{'sequence': 'The convenience and the doctors'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.3  Unnamed: 0.2  Unnamed: 0.1  Unnamed: 0                  ID  \\\n",
       "0             0             0             0           0  a2p1U000000RowfQAC   \n",
       "1             1             1             1           1  a2p1U000000RqQqQAK   \n",
       "2             2             2             2           2  a2p1U000000RqXyQAK   \n",
       "3             3             3             3           3  a2p1U000000Rq1LQAS   \n",
       "4             4             4             4           4  a2p1U000000RpiuQAC   \n",
       "\n",
       "            MEMBER_ID                                  REASONNPSSCORE__C  \\\n",
       "0  0011U00000rjFKdQAM  I showed up for my appointment, but they had m...   \n",
       "1  0011U00000riCSHQA2           Staff was polite, courteous, and on time   \n",
       "2  0011U00000riTw7QAE  Overall care is great!  It's wonderful to be a...   \n",
       "3  0011U00000rhu8eQAA  Like the doctor and staff at this location. Ea...   \n",
       "4  0011U00000rk4SHQAY                    The convenience and the doctors   \n",
       "\n",
       "                                             results  \n",
       "0  {'sequence': \"I showed up for my appointment, ...  \n",
       "1  {'sequence': 'Staff was polite, courteous, and...  \n",
       "2  {'sequence': \"Overall care is great!  It's won...  \n",
       "3  {'sequence': 'Like the doctor and staff at thi...  \n",
       "4  {'sequence': 'The convenience and the doctors'...  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF=pd.read_csv('NLPdata.csv')\n",
    "DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "25a22604",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.drop(['ID','MEMBER_ID','Unnamed: 0.3','Unnamed: 0.1','Unnamed: 0.2','Unnamed: 0.1'],inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c5ab652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "res = DF['results'].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1a906a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF['output'] = [(i.get('labels')[0]) for i in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2f495729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REASONNPSSCORE__C</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I showed up for my appointment, but they had m...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Staff was polite, courteous, and on time</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Overall care is great!  It's wonderful to be a...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Like the doctor and staff at this location. Ea...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The convenience and the doctors</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   REASONNPSSCORE__C    output\n",
       "0  I showed up for my appointment, but they had m...  positive\n",
       "1           Staff was polite, courteous, and on time  positive\n",
       "2  Overall care is great!  It's wonderful to be a...  positive\n",
       "3  Like the doctor and staff at this location. Ea...  positive\n",
       "4                    The convenience and the doctors  positive"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF1=DF[['REASONNPSSCORE__C','output']]\n",
    "DF1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8c47b212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\veereshg\\AppData\\Local\\Temp/ipykernel_4092/3826914844.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  DF1['output']=DF1['output'].map({'positive':1,'negative':0})\n"
     ]
    }
   ],
   "source": [
    "DF1['output']=DF1['output'].map({'positive':1,'negative':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "91bdca6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\veereshg\\AppData\\Local\\Temp/ipykernel_4092/1346297714.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  DF1.rename(columns={'REASONNPSSCORE__C':'review'},inplace=True)\n"
     ]
    }
   ],
   "source": [
    "DF1.rename(columns={'REASONNPSSCORE__C':'review'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0b62e353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2696\n",
       "0    1116\n",
       "Name: output, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF1['output'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b407d6b",
   "metadata": {},
   "source": [
    "Here we can notice that after using better model for labelling the data negative reviews increased from 569 to 1116\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "13ca3c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    x_data = DF1['review']       # Reviews/Input\n",
    "    y_data = DF1['output']\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    ls = WordNetLemmatizer()\n",
    "    \n",
    "    \n",
    "    # Sentiment/Output\n",
    "\n",
    "    x_data = x_data.replace({'<.*?>': ''}, regex = True)          # remove html tag\n",
    "    x_data = x_data.replace({'[^A-Za-z]': ' '}, regex = True)\n",
    "    x_data = x_data.apply(lambda review: [ls.lemmatize(w) for w in review.split() if w not in stopwords])\n",
    "    x_data = x_data.apply(lambda review: [w.lower() for w in review])\n",
    "    \n",
    "\n",
    "\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f2104fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, y_data = load_dataset()\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7c3169e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length():\n",
    "    review_length = []\n",
    "    for review in x_train:\n",
    "        review_length.append(len(review))\n",
    "\n",
    "    return int(np.ceil(np.mean(review_length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d186e107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded X Train\n",
      " [[   6    2   30 ...   22  544    0]\n",
      " [   2 1978 1130 ...  139    0    0]\n",
      " [  49   13   32 ...    0    0    0]\n",
      " ...\n",
      " [ 865 1929    0 ...    0    0    0]\n",
      " [  38   51    5 ...    0    0    0]\n",
      " [1015   27    0 ...    0    0    0]] \n",
      "\n",
      "Encoded X Test\n",
      " [[  12    9    0 ...    0    0    0]\n",
      " [  43    9    0 ...    0    0    0]\n",
      " [2031  125   10 ...  284  102  342]\n",
      " ...\n",
      " [ 144 1453   36 ...    0    0    0]\n",
      " [  11    7    0 ...    0    0    0]\n",
      " [ 843   88   13 ...    0    0    0]] \n",
      "\n",
      "Maximum review length:  12\n"
     ]
    }
   ],
   "source": [
    "# ENCODE REVIEW\n",
    "token = Tokenizer(lower=False)    # no need lower, because already lowered the data in load_data()\n",
    "token.fit_on_texts(x_train)\n",
    "x_train = token.texts_to_sequences(x_train)\n",
    "x_test = token.texts_to_sequences(x_test)\n",
    "\n",
    "max_length = get_max_length()\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')\n",
    "x_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "total_words = len(token.word_index) + 1   # add 1 because of 0 padding\n",
    "\n",
    "print('Encoded X Train\\n', x_train, '\\n')\n",
    "print('Encoded X Test\\n', x_test, '\\n')\n",
    "print('Maximum review length: ', max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a282af43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 12, 32)            116096    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 140,993\n",
      "Trainable params: 140,993\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "EMBED_DIM = 32\n",
    "LSTM_OUT = 64\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, EMBED_DIM, input_length = max_length))\n",
    "model.add(LSTM(LSTM_OUT))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "17bd6edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('LSTM1.h5',monitor='accuracy',save_best_only=True,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ccd71746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3049, 12)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "93ac398b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0663 - accuracy: 0.9790\n",
      "\n",
      "Epoch 00001: accuracy did not improve from 0.98065\n",
      "Epoch 2/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0658 - accuracy: 0.9803\n",
      "\n",
      "Epoch 00002: accuracy did not improve from 0.98065\n",
      "Epoch 3/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0597 - accuracy: 0.9826\n",
      "\n",
      "Epoch 00003: accuracy improved from 0.98065 to 0.98262, saving model to LSTM1.h5\n",
      "Epoch 4/50\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0508 - accuracy: 0.9820\n",
      "\n",
      "Epoch 00004: accuracy did not improve from 0.98262\n",
      "Epoch 5/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0467 - accuracy: 0.9862\n",
      "\n",
      "Epoch 00005: accuracy improved from 0.98262 to 0.98623, saving model to LSTM1.h5\n",
      "Epoch 6/50\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0426 - accuracy: 0.9875\n",
      "\n",
      "Epoch 00006: accuracy improved from 0.98623 to 0.98754, saving model to LSTM1.h5\n",
      "Epoch 7/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0307 - accuracy: 0.9905\n",
      "\n",
      "Epoch 00007: accuracy improved from 0.98754 to 0.99049, saving model to LSTM1.h5\n",
      "Epoch 8/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0293 - accuracy: 0.9921\n",
      "\n",
      "Epoch 00008: accuracy improved from 0.99049 to 0.99213, saving model to LSTM1.h5\n",
      "Epoch 9/50\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0286 - accuracy: 0.9921\n",
      "\n",
      "Epoch 00009: accuracy did not improve from 0.99213\n",
      "Epoch 10/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0240 - accuracy: 0.9934\n",
      "\n",
      "Epoch 00010: accuracy improved from 0.99213 to 0.99344, saving model to LSTM1.h5\n",
      "Epoch 11/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0234 - accuracy: 0.9938\n",
      "\n",
      "Epoch 00011: accuracy improved from 0.99344 to 0.99377, saving model to LSTM1.h5\n",
      "Epoch 12/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0199 - accuracy: 0.9938\n",
      "\n",
      "Epoch 00012: accuracy did not improve from 0.99377\n",
      "Epoch 13/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0176 - accuracy: 0.9961\n",
      "\n",
      "Epoch 00013: accuracy improved from 0.99377 to 0.99606, saving model to LSTM1.h5\n",
      "Epoch 14/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0121 - accuracy: 0.9967\n",
      "\n",
      "Epoch 00014: accuracy improved from 0.99606 to 0.99672, saving model to LSTM1.h5\n",
      "Epoch 15/50\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0120 - accuracy: 0.9974\n",
      "\n",
      "Epoch 00015: accuracy improved from 0.99672 to 0.99738, saving model to LSTM1.h5\n",
      "Epoch 16/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0150 - accuracy: 0.9967\n",
      "\n",
      "Epoch 00016: accuracy did not improve from 0.99738\n",
      "Epoch 17/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0217 - accuracy: 0.9934\n",
      "\n",
      "Epoch 00017: accuracy did not improve from 0.99738\n",
      "Epoch 18/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0122 - accuracy: 0.9964\n",
      "\n",
      "Epoch 00018: accuracy did not improve from 0.99738\n",
      "Epoch 19/50\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0081 - accuracy: 0.9984\n",
      "\n",
      "Epoch 00019: accuracy improved from 0.99738 to 0.99836, saving model to LSTM1.h5\n",
      "Epoch 20/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0065 - accuracy: 0.9980\n",
      "\n",
      "Epoch 00020: accuracy did not improve from 0.99836\n",
      "Epoch 21/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0054 - accuracy: 0.9990\n",
      "\n",
      "Epoch 00021: accuracy improved from 0.99836 to 0.99902, saving model to LSTM1.h5\n",
      "Epoch 22/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0038 - accuracy: 0.9993\n",
      "\n",
      "Epoch 00022: accuracy improved from 0.99902 to 0.99934, saving model to LSTM1.h5\n",
      "Epoch 23/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0079 - accuracy: 0.9977\n",
      "\n",
      "Epoch 00023: accuracy did not improve from 0.99934\n",
      "Epoch 24/50\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 0.0085 - accuracy: 0.9974\n",
      "\n",
      "Epoch 00024: accuracy did not improve from 0.99934\n",
      "Epoch 25/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0092 - accuracy: 0.9970\n",
      "\n",
      "Epoch 00025: accuracy did not improve from 0.99934\n",
      "Epoch 26/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0057 - accuracy: 0.9987\n",
      "\n",
      "Epoch 00026: accuracy did not improve from 0.99934\n",
      "Epoch 27/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0038 - accuracy: 0.9987\n",
      "\n",
      "Epoch 00027: accuracy did not improve from 0.99934\n",
      "Epoch 28/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0054 - accuracy: 0.9990: 0s - loss: 0.0024 - accura\n",
      "\n",
      "Epoch 00028: accuracy did not improve from 0.99934\n",
      "Epoch 29/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0058 - accuracy: 0.9980\n",
      "\n",
      "Epoch 00029: accuracy did not improve from 0.99934\n",
      "Epoch 30/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0021 - accuracy: 0.9997\n",
      "\n",
      "Epoch 00030: accuracy improved from 0.99934 to 0.99967, saving model to LSTM1.h5\n",
      "Epoch 31/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0019 - accuracy: 0.9997\n",
      "\n",
      "Epoch 00031: accuracy did not improve from 0.99967\n",
      "Epoch 32/50\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0015 - accuracy: 0.9997\n",
      "\n",
      "Epoch 00032: accuracy did not improve from 0.99967\n",
      "Epoch 33/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0037 - accuracy: 0.9987\n",
      "\n",
      "Epoch 00033: accuracy did not improve from 0.99967\n",
      "Epoch 34/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0182 - accuracy: 0.9941\n",
      "\n",
      "Epoch 00034: accuracy did not improve from 0.99967\n",
      "Epoch 35/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0126 - accuracy: 0.9957\n",
      "\n",
      "Epoch 00035: accuracy did not improve from 0.99967\n",
      "Epoch 36/50\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 0.0143 - accuracy: 0.9961\n",
      "\n",
      "Epoch 00036: accuracy did not improve from 0.99967\n",
      "Epoch 37/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0162 - accuracy: 0.9941\n",
      "\n",
      "Epoch 00037: accuracy did not improve from 0.99967\n",
      "Epoch 38/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0163 - accuracy: 0.9951\n",
      "\n",
      "Epoch 00038: accuracy did not improve from 0.99967\n",
      "Epoch 39/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0057 - accuracy: 0.9987\n",
      "\n",
      "Epoch 00039: accuracy did not improve from 0.99967\n",
      "Epoch 40/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0021 - accuracy: 0.9997\n",
      "\n",
      "Epoch 00040: accuracy did not improve from 0.99967\n",
      "Epoch 41/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0017 - accuracy: 0.9997\n",
      "\n",
      "Epoch 00041: accuracy did not improve from 0.99967\n",
      "Epoch 42/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0017 - accuracy: 0.9997\n",
      "\n",
      "Epoch 00042: accuracy did not improve from 0.99967\n",
      "Epoch 43/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0019 - accuracy: 0.9993\n",
      "\n",
      "Epoch 00043: accuracy did not improve from 0.99967\n",
      "Epoch 44/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0015 - accuracy: 0.9997\n",
      "\n",
      "Epoch 00044: accuracy did not improve from 0.99967\n",
      "Epoch 45/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0015 - accuracy: 0.9997\n",
      "\n",
      "Epoch 00045: accuracy did not improve from 0.99967\n",
      "Epoch 46/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0013 - accuracy: 0.9997\n",
      "\n",
      "Epoch 00046: accuracy did not improve from 0.99967\n",
      "Epoch 47/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0015 - accuracy: 0.9997\n",
      "\n",
      "Epoch 00047: accuracy did not improve from 0.99967\n",
      "Epoch 48/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0014 - accuracy: 0.9997\n",
      "\n",
      "Epoch 00048: accuracy did not improve from 0.99967\n",
      "Epoch 49/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0013 - accuracy: 0.9997\n",
      "\n",
      "Epoch 00049: accuracy did not improve from 0.99967\n",
      "Epoch 50/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 0.0013 - accuracy: 0.9997\n",
      "\n",
      "Epoch 00050: accuracy did not improve from 0.99967\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1df000a9370>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size = 64, epochs = 50, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "dd2822b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\veereshg\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:454: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Prediction: 611\n",
      "Wrong Prediction: 152\n",
      "Accuracy: 80.07863695937091\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_classes(x_test, batch_size = 128)\n",
    "\n",
    "true = 0\n",
    "for i, y in enumerate(y_test):\n",
    "    if y == y_pred[i]:\n",
    "        true += 1\n",
    "\n",
    "print('Correct Prediction: {}'.format(true))\n",
    "print('Wrong Prediction: {}'.format(len(y_pred) - true))\n",
    "print('Accuracy: {}'.format(true/len(y_pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9f8224e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = load_model('LSTM1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "dc81631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1=model2.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "427f7714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.59      0.62       218\n",
      "           1       0.84      0.88      0.86       545\n",
      "\n",
      "    accuracy                           0.80       763\n",
      "   macro avg       0.75      0.73      0.74       763\n",
      "weighted avg       0.79      0.80      0.79       763\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c156e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1321039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cc3faa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3794bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96b614b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecdb0a74",
   "metadata": {},
   "source": [
    "\n",
    "# After downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "e6ce25b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF=pd.read_csv('NLPdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "d650df30",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.drop(['ID','MEMBER_ID','Unnamed: 0.3','Unnamed: 0.1','Unnamed: 0.2','Unnamed: 0.1'],inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "bec05233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "res = DF['results'].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "fdd6023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF['output'] = [(i.get('labels')[0]) for i in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "43944efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>REASONNPSSCORE__C</th>\n",
       "      <th>results</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I showed up for my appointment, but they had m...</td>\n",
       "      <td>{'sequence': \"I showed up for my appointment, ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Staff was polite, courteous, and on time</td>\n",
       "      <td>{'sequence': 'Staff was polite, courteous, and...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Overall care is great!  It's wonderful to be a...</td>\n",
       "      <td>{'sequence': \"Overall care is great!  It's won...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Like the doctor and staff at this location. Ea...</td>\n",
       "      <td>{'sequence': 'Like the doctor and staff at thi...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The convenience and the doctors</td>\n",
       "      <td>{'sequence': 'The convenience and the doctors'...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                  REASONNPSSCORE__C  \\\n",
       "0           0  I showed up for my appointment, but they had m...   \n",
       "1           1           Staff was polite, courteous, and on time   \n",
       "2           2  Overall care is great!  It's wonderful to be a...   \n",
       "3           3  Like the doctor and staff at this location. Ea...   \n",
       "4           4                    The convenience and the doctors   \n",
       "\n",
       "                                             results    output  \n",
       "0  {'sequence': \"I showed up for my appointment, ...  positive  \n",
       "1  {'sequence': 'Staff was polite, courteous, and...  positive  \n",
       "2  {'sequence': \"Overall care is great!  It's won...  positive  \n",
       "3  {'sequence': 'Like the doctor and staff at thi...  positive  \n",
       "4  {'sequence': 'The convenience and the doctors'...  positive  "
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "33fdd75c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REASONNPSSCORE__C</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I showed up for my appointment, but they had m...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Staff was polite, courteous, and on time</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Overall care is great!  It's wonderful to be a...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Like the doctor and staff at this location. Ea...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The convenience and the doctors</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   REASONNPSSCORE__C    output\n",
       "0  I showed up for my appointment, but they had m...  positive\n",
       "1           Staff was polite, courteous, and on time  positive\n",
       "2  Overall care is great!  It's wonderful to be a...  positive\n",
       "3  Like the doctor and staff at this location. Ea...  positive\n",
       "4                    The convenience and the doctors  positive"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF1=DF[['REASONNPSSCORE__C','output']]\n",
    "DF1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "bcf8a69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\veereshg\\AppData\\Local\\Temp/ipykernel_4092/3826914844.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  DF1['output']=DF1['output'].map({'positive':1,'negative':0})\n"
     ]
    }
   ],
   "source": [
    "DF1['output']=DF1['output'].map({'positive':1,'negative':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "0465e581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2696, 2)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_review = DF1[DF1['output'] == 1]\n",
    "neg_review  = DF1[DF1['output'] == 0]\n",
    "pos_review.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "910f27f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1116, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "pos_downsample = resample(pos_review,\n",
    "             replace=True,\n",
    "             n_samples=len(neg_review ),\n",
    "             random_state=42)\n",
    "\n",
    "print(pos_downsample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "57d347ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_downsampled = pd.concat([pos_downsample, neg_review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "a574e3bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REASONNPSSCORE__C</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1223</th>\n",
       "      <td>Customer service is great and dr was very help...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1846</th>\n",
       "      <td>Just started with them, so far so good</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>Save money, convenient</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1558</th>\n",
       "      <td>I love my care provider - Ashley Giles.  She r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2331</th>\n",
       "      <td>I was in the office for one symptom, however a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3795</th>\n",
       "      <td>While a good concept and convenient location, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3798</th>\n",
       "      <td>I sometimes feel as though the Dr is trying to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3804</th>\n",
       "      <td>Nurse was unable to complete a blood draw beca...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3808</th>\n",
       "      <td>Very skeptical that you will soon be without a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3811</th>\n",
       "      <td>This was without a doubt the worst example of ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2232 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      REASONNPSSCORE__C  output\n",
       "1223  Customer service is great and dr was very help...       1\n",
       "1846             Just started with them, so far so good       1\n",
       "1600                             Save money, convenient       1\n",
       "1558  I love my care provider - Ashley Giles.  She r...       1\n",
       "2331  I was in the office for one symptom, however a...       1\n",
       "...                                                 ...     ...\n",
       "3795  While a good concept and convenient location, ...       0\n",
       "3798  I sometimes feel as though the Dr is trying to...       0\n",
       "3804  Nurse was unable to complete a blood draw beca...       0\n",
       "3808  Very skeptical that you will soon be without a...       0\n",
       "3811  This was without a doubt the worst example of ...       0\n",
       "\n",
       "[2232 rows x 2 columns]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f29c45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "ef68f904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    x_data = data_downsampled['REASONNPSSCORE__C']     # Reviews/Input\n",
    "    y_data = data_downsampled['output']  \n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    # Sentiment/Output\n",
    "    ls = WordNetLemmatizer()\n",
    "    # PRE-PROCESS REVIEW\n",
    "    #x_data = x_data.replace({'<.*?>': ''}, regex = True)          # remove html tag\n",
    "    x_data = x_data.replace({'[^A-Za-z]': ' '}, regex = True)     # remove non alphabet\n",
    "    x_data = x_data.apply(lambda review: [ls.lemmatize(w)  for w in review.split() if w not in stopwords])  # remove stop words\n",
    "    x_data = x_data.apply(lambda review: [w.lower() for w in review])   # lower case\n",
    "    \n",
    "\n",
    "\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "fa27490d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, y_data = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "ccda231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "2d4e615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length():\n",
    "    review_length = []\n",
    "    for review in x_train:\n",
    "        review_length.append(len(review))\n",
    "\n",
    "    return int(np.ceil(np.mean(review_length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "cf4034f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded X Train\n",
      " [[ 143 1895  865 ...    0    0    0]\n",
      " [ 260  756    0 ...    0    0    0]\n",
      " [   1   38    8 ... 1896  569   37]\n",
      " ...\n",
      " [  17  138    7 ...  165   13   70]\n",
      " [  25    9  101 ...    0    0    0]\n",
      " [ 593  129  182 ...    0    0    0]] \n",
      "\n",
      "Encoded X Test\n",
      " [[ 120    2   10 ...    0    0    0]\n",
      " [  17   51   14 ...    0    0    0]\n",
      " [ 128    0    0 ...    0    0    0]\n",
      " ...\n",
      " [ 450  164  336 ...    0    0    0]\n",
      " [  64  935   90 ...    0    0    0]\n",
      " [ 121    2 2842 ...    0    0    0]] \n",
      "\n",
      "Maximum review length:  15\n"
     ]
    }
   ],
   "source": [
    "token = Tokenizer(lower=False)    # no need lower, because already lowered the data in load_data()\n",
    "token.fit_on_texts(x_train)\n",
    "x_train = token.texts_to_sequences(x_train)\n",
    "x_test = token.texts_to_sequences(x_test)\n",
    "\n",
    "max_length = get_max_length()\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')\n",
    "x_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "total_words = len(token.word_index) + 1   # add 1 because of 0 padding\n",
    "\n",
    "print('Encoded X Train\\n', x_train, '\\n')\n",
    "print('Encoded X Test\\n', x_test, '\\n')\n",
    "print('Maximum review length: ', max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "2aa78c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('LSTM3.h5',monitor='accuracy',save_best_only=True,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "960259b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 15, 32)            110976    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 135,873\n",
      "Trainable params: 135,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "EMBED_DIM = 32\n",
    "LSTM_OUT = 64\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, EMBED_DIM, input_length = max_length))\n",
    "model.add(LSTM(LSTM_OUT))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "0cda83f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "33/33 [==============================] - 3s 9ms/step - loss: 0.6431 - accuracy: 0.6668\n",
      "\n",
      "Epoch 00001: accuracy improved from -inf to 0.66683, saving model to LSTM3.h5\n",
      "Epoch 2/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.4307 - accuracy: 0.8042\n",
      "\n",
      "Epoch 00002: accuracy improved from 0.66683 to 0.80419, saving model to LSTM3.h5\n",
      "Epoch 3/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.2638 - accuracy: 0.8948\n",
      "\n",
      "Epoch 00003: accuracy improved from 0.80419 to 0.89479, saving model to LSTM3.h5\n",
      "Epoch 4/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.1849 - accuracy: 0.9362\n",
      "\n",
      "Epoch 00004: accuracy improved from 0.89479 to 0.93619, saving model to LSTM3.h5\n",
      "Epoch 5/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.1368 - accuracy: 0.9537\n",
      "\n",
      "Epoch 00005: accuracy improved from 0.93619 to 0.95373, saving model to LSTM3.h5\n",
      "Epoch 6/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0965 - accuracy: 0.9713\n",
      "\n",
      "Epoch 00006: accuracy improved from 0.95373 to 0.97126, saving model to LSTM3.h5\n",
      "Epoch 7/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0760 - accuracy: 0.9791\n",
      "\n",
      "Epoch 00007: accuracy improved from 0.97126 to 0.97906, saving model to LSTM3.h5\n",
      "Epoch 8/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0594 - accuracy: 0.9825\n",
      "\n",
      "Epoch 00008: accuracy improved from 0.97906 to 0.98246, saving model to LSTM3.h5\n",
      "Epoch 9/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0534 - accuracy: 0.9864\n",
      "\n",
      "Epoch 00009: accuracy improved from 0.98246 to 0.98636, saving model to LSTM3.h5\n",
      "Epoch 10/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0548 - accuracy: 0.9878\n",
      "\n",
      "Epoch 00010: accuracy improved from 0.98636 to 0.98782, saving model to LSTM3.h5\n",
      "Epoch 11/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0538 - accuracy: 0.9873\n",
      "\n",
      "Epoch 00011: accuracy did not improve from 0.98782\n",
      "Epoch 12/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0381 - accuracy: 0.9917\n",
      "\n",
      "Epoch 00012: accuracy improved from 0.98782 to 0.99172, saving model to LSTM3.h5\n",
      "Epoch 13/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0589 - accuracy: 0.9844\n",
      "\n",
      "Epoch 00013: accuracy did not improve from 0.99172\n",
      "Epoch 14/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0533 - accuracy: 0.9868\n",
      "\n",
      "Epoch 00014: accuracy did not improve from 0.99172\n",
      "Epoch 15/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0373 - accuracy: 0.9903\n",
      "\n",
      "Epoch 00015: accuracy did not improve from 0.99172\n",
      "Epoch 16/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0321 - accuracy: 0.9932\n",
      "\n",
      "Epoch 00016: accuracy improved from 0.99172 to 0.99318, saving model to LSTM3.h5\n",
      "Epoch 17/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0337 - accuracy: 0.9927\n",
      "\n",
      "Epoch 00017: accuracy did not improve from 0.99318\n",
      "Epoch 18/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0315 - accuracy: 0.9922\n",
      "\n",
      "Epoch 00018: accuracy did not improve from 0.99318\n",
      "Epoch 19/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0389 - accuracy: 0.9903\n",
      "\n",
      "Epoch 00019: accuracy did not improve from 0.99318\n",
      "Epoch 20/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0470 - accuracy: 0.9883\n",
      "\n",
      "Epoch 00020: accuracy did not improve from 0.99318\n",
      "Epoch 21/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0281 - accuracy: 0.9946\n",
      "\n",
      "Epoch 00021: accuracy improved from 0.99318 to 0.99464, saving model to LSTM3.h5\n",
      "Epoch 22/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0221 - accuracy: 0.9961\n",
      "\n",
      "Epoch 00022: accuracy improved from 0.99464 to 0.99610, saving model to LSTM3.h5\n",
      "Epoch 23/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0214 - accuracy: 0.9971\n",
      "\n",
      "Epoch 00023: accuracy improved from 0.99610 to 0.99708, saving model to LSTM3.h5\n",
      "Epoch 24/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0205 - accuracy: 0.9971\n",
      "\n",
      "Epoch 00024: accuracy did not improve from 0.99708\n",
      "Epoch 25/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0203 - accuracy: 0.9971\n",
      "\n",
      "Epoch 00025: accuracy did not improve from 0.99708\n",
      "Epoch 26/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0199 - accuracy: 0.9971\n",
      "\n",
      "Epoch 00026: accuracy did not improve from 0.99708\n",
      "Epoch 27/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0268 - accuracy: 0.9932\n",
      "\n",
      "Epoch 00027: accuracy did not improve from 0.99708\n",
      "Epoch 28/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0368 - accuracy: 0.9912\n",
      "\n",
      "Epoch 00028: accuracy did not improve from 0.99708\n",
      "Epoch 29/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0253 - accuracy: 0.9951\n",
      "\n",
      "Epoch 00029: accuracy did not improve from 0.99708\n",
      "Epoch 30/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0237 - accuracy: 0.9961\n",
      "\n",
      "Epoch 00030: accuracy did not improve from 0.99708\n",
      "Epoch 31/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0210 - accuracy: 0.9966\n",
      "\n",
      "Epoch 00031: accuracy did not improve from 0.99708\n",
      "Epoch 32/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0198 - accuracy: 0.9971\n",
      "\n",
      "Epoch 00032: accuracy did not improve from 0.99708\n",
      "Epoch 33/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0196 - accuracy: 0.9971\n",
      "\n",
      "Epoch 00033: accuracy did not improve from 0.99708\n",
      "Epoch 34/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0297 - accuracy: 0.9937\n",
      "\n",
      "Epoch 00034: accuracy did not improve from 0.99708\n",
      "Epoch 35/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0245 - accuracy: 0.9951\n",
      "\n",
      "Epoch 00035: accuracy did not improve from 0.99708\n",
      "Epoch 36/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0199 - accuracy: 0.9971\n",
      "\n",
      "Epoch 00036: accuracy did not improve from 0.99708\n",
      "Epoch 37/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0194 - accuracy: 0.9971\n",
      "\n",
      "Epoch 00037: accuracy did not improve from 0.99708\n",
      "Epoch 38/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0196 - accuracy: 0.9971\n",
      "\n",
      "Epoch 00038: accuracy did not improve from 0.99708\n",
      "Epoch 39/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0201 - accuracy: 0.9971\n",
      "\n",
      "Epoch 00039: accuracy did not improve from 0.99708\n",
      "Epoch 40/50\n",
      "33/33 [==============================] - 0s 9ms/step - loss: 0.0269 - accuracy: 0.9951\n",
      "\n",
      "Epoch 00040: accuracy did not improve from 0.99708\n",
      "Epoch 41/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0206 - accuracy: 0.9966\n",
      "\n",
      "Epoch 00041: accuracy did not improve from 0.99708\n",
      "Epoch 42/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0188 - accuracy: 0.9971\n",
      "\n",
      "Epoch 00042: accuracy did not improve from 0.99708\n",
      "Epoch 43/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0190 - accuracy: 0.9971\n",
      "\n",
      "Epoch 00043: accuracy did not improve from 0.99708\n",
      "Epoch 44/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0173 - accuracy: 0.9971\n",
      "\n",
      "Epoch 00044: accuracy did not improve from 0.99708\n",
      "Epoch 45/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0468 - accuracy: 0.9864\n",
      "\n",
      "Epoch 00045: accuracy did not improve from 0.99708\n",
      "Epoch 46/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0257 - accuracy: 0.9942\n",
      "\n",
      "Epoch 00046: accuracy did not improve from 0.99708\n",
      "Epoch 47/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0191 - accuracy: 0.9971\n",
      "\n",
      "Epoch 00047: accuracy did not improve from 0.99708\n",
      "Epoch 48/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0193 - accuracy: 0.9961\n",
      "\n",
      "Epoch 00048: accuracy did not improve from 0.99708\n",
      "Epoch 49/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0174 - accuracy: 0.9971\n",
      "\n",
      "Epoch 00049: accuracy did not improve from 0.99708\n",
      "Epoch 50/50\n",
      "33/33 [==============================] - 0s 8ms/step - loss: 0.0178 - accuracy: 0.9966\n",
      "\n",
      "Epoch 00050: accuracy did not improve from 0.99708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1df00089040>"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size = 64, epochs = 50, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "7645ac87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\veereshg\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:454: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Prediction: 144\n",
      "Wrong Prediction: 35\n",
      "Accuracy: 80.44692737430168\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_classes(x_test, batch_size = 128)\n",
    "\n",
    "true = 0\n",
    "for i, y in enumerate(y_test):\n",
    "    if y == y_pred[i]:\n",
    "        true += 1\n",
    "\n",
    "print('Correct Prediction: {}'.format(true))\n",
    "print('Wrong Prediction: {}'.format(len(y_pred) - true))\n",
    "print('Accuracy: {}'.format(true/len(y_pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "d76b7a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = load_model('LSTM3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "af6327d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 32 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001DF001F44C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "y_pred1=model3.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "538e7fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.78      0.80        87\n",
      "           1       0.80      0.84      0.82        92\n",
      "\n",
      "    accuracy                           0.81       179\n",
      "   macro avg       0.81      0.81      0.81       179\n",
      "weighted avg       0.81      0.81      0.81       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d67b99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6808e735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e288204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ef589e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81827488",
   "metadata": {},
   "source": [
    "# CREATING WORD EMBEDDING FOR THE SENTENCES USING PRETRAINED BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba6c5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Below iS the dataframe which has the word embedings for the given dataset\n",
    "#Here BERT model creates  embeddings for the given sentence using cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "18b04bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_EMB=pd.read_csv('EMBEDED.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "07a67c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>REASONNPSSCORE__C</th>\n",
       "      <th>output</th>\n",
       "      <th>emb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I showed up for my appointment, but they had m...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[-0.09676336  0.23733687 -0.03380208 -0.175840...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Staff was polite, courteous, and on time</td>\n",
       "      <td>positive</td>\n",
       "      <td>[ 0.04106916  0.25190938 -0.12936194 -0.085945...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Overall care is great!  It's wonderful to be a...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[-6.6784762e-02  8.1635922e-02 -1.8907736e-01 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Like the doctor and staff at this location. Ea...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[-2.04459503e-02  3.02443504e-01  2.63532002e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The convenience and the doctors</td>\n",
       "      <td>positive</td>\n",
       "      <td>[-0.17510815  0.36582455 -0.3491363  -0.244227...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                  REASONNPSSCORE__C    output  \\\n",
       "0           0  I showed up for my appointment, but they had m...  positive   \n",
       "1           1           Staff was polite, courteous, and on time  positive   \n",
       "2           2  Overall care is great!  It's wonderful to be a...  positive   \n",
       "3           3  Like the doctor and staff at this location. Ea...  positive   \n",
       "4           4                    The convenience and the doctors  positive   \n",
       "\n",
       "                                                 emb  \n",
       "0  [-0.09676336  0.23733687 -0.03380208 -0.175840...  \n",
       "1  [ 0.04106916  0.25190938 -0.12936194 -0.085945...  \n",
       "2  [-6.6784762e-02  8.1635922e-02 -1.8907736e-01 ...  \n",
       "3  [-2.04459503e-02  3.02443504e-01  2.63532002e-...  \n",
       "4  [-0.17510815  0.36582455 -0.3491363  -0.244227...  "
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_EMB.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "0fd63712",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_EMB=DF_EMB[['emb','output']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "481747c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_EMB['emb'] = DF_EMB['emb'].apply(lambda x: np.array(x.replace('[','').replace(']','').split()).astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "95d08d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [-0.09676336, 0.23733687, -0.03380208, -0.1758...\n",
       "1       [0.04106916, 0.25190938, -0.12936194, -0.08594...\n",
       "2       [-0.066784762, 0.081635922, -0.18907736, 0.120...\n",
       "3       [-0.0204459503, 0.302443504, 2.63532002e-05, 0...\n",
       "4       [-0.17510815, 0.36582455, -0.3491363, -0.24422...\n",
       "                              ...                        \n",
       "3807    [-0.54986167, -0.7347374, -0.29466134, 0.34458...\n",
       "3808    [-0.12956539, -0.17287686, -0.14482819, 0.0309...\n",
       "3809    [-0.06594059, 0.14354283, -0.25912088, -0.4730...\n",
       "3810    [0.04620619, 0.08419552, 0.6763907, -0.1371661...\n",
       "3811    [-0.14128962, 0.03340659, 0.05415522, 0.021991...\n",
       "Name: emb, Length: 3812, dtype: object"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_EMB['emb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "0f9c3eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_EMB['output']=DF_EMB['output'].map({'positive':1,'negative':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "d091a4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data=DF_EMB['emb']\n",
    "y_data=DF_EMB['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "03cfd515",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee106789",
   "metadata": {},
   "source": [
    "# Training the dataset on SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "421b3841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "X =np.array(x_train.tolist())\n",
    "y = y_train\n",
    "clf = svm.SVC()\n",
    "clf.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "6556dfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=clf.predict(np.array(x_test.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "36ff0634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.67      0.68       221\n",
      "           1       0.87      0.88      0.87       542\n",
      "\n",
      "    accuracy                           0.82       763\n",
      "   macro avg       0.78      0.77      0.78       763\n",
      "weighted avg       0.82      0.82      0.82       763\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42668f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986c7368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98aed37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdee7f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee20148",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
